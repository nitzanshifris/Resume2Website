#!/usr/bin/env python3
"""
Test Full CV2WEB Pipeline
Tests the complete automatic processing pipeline
"""
import asyncio
import sys
import os

# Add backend to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../apps/backend'))

async def test_full_pipeline():
    """Test the complete pipeline"""
    print("üß™ Testing Full CV2WEB Pipeline")
    print("=" * 50)
    
    try:
        # Test 1: Model Router availability
        print("1. Testing Model Router...")
        from llm_handler.base_handler import model_router, test_model_router
        
        router_available = test_model_router()
        if not router_available:
            print("‚ùå Model Router not available. Start it with: ./run_model_router.sh")
            return False
        
        # Test 2: Component Selector
        print("\n2. Testing Component Selector...")
        from llm_handler.component_selector_enhanced import component_selector
        
        mock_cv_data = {
            "data": {
                "structured_components": [
                    {
                        "component_type": "HeroSection",
                        "data": {"fullName": "John Doe", "professionalTitle": "Software Engineer"}
                    },
                    {
                        "component_type": "ExperienceSection",
                        "data": {"experienceItems": [{"jobTitle": "Senior Developer"}]}
                    }
                ]
            }
        }
        
        selection_result = component_selector.select_components(mock_cv_data)
        if selection_result["success"]:
            print(f"‚úÖ Component selection: {selection_result['archetype']}, {len(selection_result['selected_components'])} components")
        else:
            print(f"‚ùå Component selection failed: {selection_result.get('error')}")
            return False
        
        # Test 3: Prompt Generator
        print("\n3. Testing Prompt Generator...")
        from llm_handler.prompt_generator import prompt_generator
        
        prompt_result = prompt_generator.generate_portfolio_prompt(
            cv_data=mock_cv_data,
            job_id="test_job"
        )
        
        if prompt_result["success"]:
            print(f"‚úÖ Prompt generation: template {prompt_result['metadata']['template_id']}")
            print(f"   Formatted data keys: {list(prompt_result['formatted_data'].keys())}")
        else:
            print(f"‚ùå Prompt generation failed: {prompt_result.get('error')}")
            return False
        
        # Test 4: Adapter Integration
        print("\n4. Testing Adapter Integration...")
        from generators.adapter_integration import AdapterIntegration
        
        adapter_integration = AdapterIntegration()
        transformed_data = await adapter_integration.transform_cv_data(
            cv_data=mock_cv_data,
            selected_components=selection_result["selected_components"],
            archetype=selection_result["archetype"]
        )
        
        if transformed_data.get("metadata", {}).get("transformation_complete"):
            print(f"‚úÖ Adapter integration: {len(transformed_data['sections'])} sections transformed")
        else:
            print(f"‚ùå Adapter integration failed")
            return False
        
        # Test 5: Enhanced Website Generator (mock)
        print("\n5. Testing Enhanced Website Generator...")
        from llm_handler.enhanced_website_generator import enhanced_website_generator
        
        # We won't actually generate a full website in test, just check if it loads
        print("‚úÖ Enhanced website generator loaded successfully")
        
        # Test 6: Recommendation Logger
        print("\n6. Testing Recommendation Logger...")
        from services.recommendation_logger import recommendation_logger
        
        # Test logging a recommendation
        rec_id = recommendation_logger.log_recommendation(
            job_id="test_job",
            cv_data=mock_cv_data,
            recommendation={
                "selected_components": selection_result["selected_components"],
                "metadata": prompt_result["metadata"]
            }
        )
        
        if rec_id:
            print(f"‚úÖ Recommendation logged: {rec_id}")
            
            # Test feedback logging
            recommendation_logger.log_feedback(
                recommendation_id=rec_id,
                feedback_type="pipeline_test",
                feedback_data={"test": True, "success": True}
            )
            print("‚úÖ Feedback logging successful")
        else:
            print("‚ùå Recommendation logging failed")
            return False
        
        print("\n" + "=" * 50)
        print("üéâ Full Pipeline Test Completed Successfully!")
        print("\nüìä Pipeline Components:")
        print("   ‚úÖ Model Router - Intelligent model selection")
        print("   ‚úÖ Component Selector - Archetype-based component selection")
        print("   ‚úÖ Prompt Generator - Advanced prompt generation with design systems")
        print("   ‚úÖ Adapter Integration - Data transformation")
        print("   ‚úÖ Enhanced Website Generator - React project generation")
        print("   ‚úÖ Recommendation Logger - Analytics and feedback tracking")
        
        print("\nüöÄ System ready for full automatic CV processing!")
        print("   Upload a CV at: http://localhost:3000")
        print("   Watch the magic happen automatically!")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Pipeline test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(test_full_pipeline())
    sys.exit(0 if success else 1)